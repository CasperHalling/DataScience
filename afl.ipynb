{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/100000 [00:12<87:50:28,  3.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(test)\n\u001b[0;32m     41\u001b[0m \u001b[39m# Clean the data\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m data \u001b[39m=\u001b[39m clean_content(data)\n\u001b[0;32m     44\u001b[0m \u001b[39m# Save the cleaned data to a new CSV file\u001b[39;00m\n\u001b[0;32m     45\u001b[0m data\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mnews_cleaned.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mclean_content\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_content\u001b[39m(inp):\n\u001b[0;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inp))):\n\u001b[0;32m     20\u001b[0m         \u001b[39m# Converting all content to lower case letters.\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39;49mapplymap(\u001b[39mlambda\u001b[39;49;00m x:x\u001b[39m.\u001b[39;49mlower() \u001b[39mif\u001b[39;49;00m \u001b[39mtype\u001b[39;49m(x) \u001b[39m==\u001b[39;49m \u001b[39mstr\u001b[39;49m \u001b[39melse\u001b[39;49;00m x)\n\u001b[0;32m     23\u001b[0m         \u001b[39m# Uses regular expressions to remove and or substitute unwanted substrings with dummy substrings.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[39m# Removes all newlines and tabs\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         inp\u001b[39m.\u001b[39mat[i,\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mt]*\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, inp\u001b[39m.\u001b[39mat[i,\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9653\u001b[0m, in \u001b[0;36mDataFrame.applymap\u001b[1;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[0;32m   9650\u001b[0m         \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer(x, func, ignore_na\u001b[39m=\u001b[39mignore_na)\n\u001b[0;32m   9651\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer(x\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values, func, ignore_na\u001b[39m=\u001b[39mignore_na)\n\u001b[1;32m-> 9653\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(infer)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mapplymap\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9651\u001b[0m, in \u001b[0;36mDataFrame.applymap.<locals>.infer\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   9649\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mempty:\n\u001b[0;32m   9650\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer(x, func, ignore_na\u001b[39m=\u001b[39mignore_na)\n\u001b[1;32m-> 9651\u001b[0m \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(x\u001b[39m.\u001b[39;49mastype(\u001b[39mobject\u001b[39;49m)\u001b[39m.\u001b[39;49m_values, func, ignore_na\u001b[39m=\u001b[39;49mignore_na)\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mclean_content.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_content\u001b[39m(inp):\n\u001b[0;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inp))):\n\u001b[0;32m     20\u001b[0m         \u001b[39m# Converting all content to lower case letters.\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mapplymap(\u001b[39mlambda\u001b[39;00m x:x\u001b[39m.\u001b[39mlower() \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(x) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m \u001b[39melse\u001b[39;00m x)\n\u001b[0;32m     23\u001b[0m         \u001b[39m# Uses regular expressions to remove and or substitute unwanted substrings with dummy substrings.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[39m# Removes all newlines and tabs\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         inp\u001b[39m.\u001b[39mat[i,\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mt]*\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, inp\u001b[39m.\u001b[39mat[i,\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reads the csv-file into a pandas dataframe\n",
    "test = pd.read_csv('news_cleaned_2018_02_13.csv', chunksize=100000)\n",
    "\n",
    "# Initialize nltk's PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Cleans the data\n",
    "def clean_content(inp):\n",
    "    for i in tqdm(range(len(inp))):\n",
    "        # Converting all content to lower case letters.\n",
    "        inp = inp.applymap(lambda x:x.lower() if type(x) == str else x)\n",
    "\n",
    "        # Uses regular expressions to remove and or substitute unwanted substrings with dummy substrings.\n",
    "        # Removes all newlines and tabs\n",
    "        inp.at[i,'content'] = re.sub(r\"[\\n\\t]*\", \"\", inp.at[i,'content'])\n",
    "        # Removes all whitespace that is instantly after a whitespace\n",
    "        inp.at[i,'content'] = re.sub(r\"[\\s]{2,}\", \"\", inp.at[i,'content'])\n",
    "        # Sub of dates\n",
    "        inp.at[i,'content'] = re.sub(r\"(([a-zA-Z]*)(\\s+)(\\d{2,})(,{1})(\\s+)(\\d{2,4}))\",\"uniquedate\", inp.at[i,'content'], flags=re.MULTILINE)\n",
    "        # Sub of emails\n",
    "        inp.at[i,'content'] = re.sub(r\"([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)\", \"uniqueemail\", inp.at[i,'content'], flags=re.MULTILINE)\n",
    "        # Sub of url's\n",
    "        inp.at[i,'content'] = re.sub(r\"(?:https?:\\/\\/)?(?:www\\.)?([^@\\s]+\\.[a-zA-Z]{2,4})[^\\s]*\",\"uniqueurl\", inp.at[i,'content'], flags=re.MULTILINE)\n",
    "        #Removal of numbers - (\\s)\\$?(?:[\\d,.-])+\n",
    "        inp.at[i,'content'] = re.sub(r\"\\b(\\d+)\\b\",\"uniquenum\", inp.at[i,'content'], flags=re.MULTILINE)\n",
    "    return inp\n",
    "\n",
    "\n",
    "data = next(test)\n",
    "\n",
    "# Clean the data\n",
    "data = clean_content(data)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "data.to_csv('news_cleaned.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size before removing stopwords: 862462\n",
      "Vocabulary size after removing stopwords: 862165\n",
      "Reduction rate after removing stopwords: 0.03%\n",
      "Vocabulary size after stemming: 794997\n",
      "Reduction rate after stemming: 7.79%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "\n",
    "# read in the data and sample 10% of the rows\n",
    "data = pd.read_csv('news_cleaned.csv').sample(frac=0.1)\n",
    "\n",
    "# Tokenize the text\n",
    "data['tokens'] = data['content'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Remove stopwords and compute the size of the vocabulary\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "vocab_size = len(set([word for row in data['tokens'] for word in row]))\n",
    "reduction_rate_stopwords = (1 - (vocab_size / len(set([word for row in data['content'] for word in row.split()])))) * 100\n",
    "\n",
    "# Remove word variations with stemming and compute the size of the vocabulary\n",
    "ps = PorterStemmer()\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [ps.stem(word) for word in x])\n",
    "vocab_size_stem = len(set([word for row in data['tokens'] for word in row]))\n",
    "reduction_rate_stemming = (1 - (vocab_size_stem / vocab_size)) * 100\n",
    "\n",
    "print(f\"Vocabulary size before removing stopwords: {len(set([word for row in data['content'] for word in row.split()]))}\")\n",
    "print(f\"Vocabulary size after removing stopwords: {vocab_size}\")\n",
    "print(f\"Reduction rate after removing stopwords: {reduction_rate_stopwords:.2f}%\")\n",
    "print(f\"Vocabulary size after stemming: {vocab_size_stem}\")\n",
    "print(f\"Reduction rate after stemming: {reduction_rate_stemming:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "num_rows = len(data)\n",
    "indices = np.arange(num_rows)\n",
    "np.random.shuffle(indices)\n",
    "train_idx = indices[:int(train_ratio*num_rows)]\n",
    "val_idx = indices[int(train_ratio*num_rows):int((train_ratio+val_ratio)*num_rows)]\n",
    "test_idx = indices[int((train_ratio+val_ratio)*num_rows):]\n",
    "\n",
    "train_data = data.iloc[train_idx]\n",
    "val_data = data.iloc[val_idx]\n",
    "test_data = data.iloc[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Baseline Accuracy: 0.243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_6732\\96736283.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['predicted'] = [random.choice(['reliable', 'fake']) for _ in range(len(data))]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def random_baseline(data):\n",
    "    data['predicted'] = [random.choice(['reliable', 'fake']) for _ in range(len(data))]\n",
    "    return data\n",
    "\n",
    "def calculate_accuracy(data, true_labels):\n",
    "    correct_predictions = data[data['predicted'] == true_labels]\n",
    "    accuracy = len(correct_predictions) / len(data)\n",
    "    return accuracy\n",
    "\n",
    "# Example usage:\n",
    "random_data = random_baseline(test_data)\n",
    "accuracy = calculate_accuracy(random_data, test_data['type'])\n",
    "print(\"Random Baseline Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_6732\\96736283.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['predicted'] = [random.choice(['reliable', 'fake']) for _ in range(len(data))]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m y_true \u001b[39m=\u001b[39m test_data[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m      7\u001b[0m y_pred \u001b[39m=\u001b[39m random_data[\u001b[39m'\u001b[39m\u001b[39mpredicted\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mvalues\n\u001b[1;32m----> 8\u001b[0m report \u001b[39m=\u001b[39m classification_report(y_true, y_pred)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mClassification report for random baseline:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, report)\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2310\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassification_report\u001b[39m(\n\u001b[0;32m   2196\u001b[0m     y_true,\n\u001b[0;32m   2197\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2205\u001b[0m ):\n\u001b[0;32m   2206\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m \n\u001b[0;32m   2208\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2307\u001b[0m \u001b[39m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2310\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   2312\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2313\u001b[0m         labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:87\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m---> 87\u001b[0m type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my_true\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     88\u001b[0m type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m y_type \u001b[39m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:386\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m# Check multiclass\u001b[39;00m\n\u001b[0;32m    385\u001b[0m first_row \u001b[39m=\u001b[39m y[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(y) \u001b[39melse\u001b[39;00m y\u001b[39m.\u001b[39mgetrow(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdata\n\u001b[1;32m--> 386\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39;49munique_values(y)\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m (y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(first_row) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    387\u001b[0m     \u001b[39m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m suffix\n\u001b[0;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:84\u001b[0m, in \u001b[0;36m_NumPyApiWrapper.unique_values\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munique_values\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy\u001b[39m.\u001b[39;49munique(x)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     ar\u001b[39m.\u001b[39;49msort()\n\u001b[0;32m    337\u001b[0m     aux \u001b[39m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mbool_)\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate random predictions\n",
    "random_data = random_baseline(test_data)\n",
    "\n",
    "y_true = test_data['type'].values\n",
    "y_pred = random_data['predicted'].astype(str).values\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Classification report for random baseline:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Label model:\n",
      "Training accuracy: 0.492\n",
      "Validation accuracy: 0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_34916\\2598610915.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['predicted'] = mcl\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_34916\\2598610915.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_data['predicted'] = mcl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"news_cleaned.csv\")\n",
    "\n",
    "# Define train, validation, and test sets\n",
    "train_size = int(len(data) * 0.6)\n",
    "val_size = int(len(data) * 0.2)\n",
    "test_size = len(data) - train_size - val_size\n",
    "\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size + val_size]\n",
    "test_data = data[train_size + val_size:]\n",
    "\n",
    "# Define Most Common Label model\n",
    "def MCL(train_data, val_data):\n",
    "    # Get most common label from training set\n",
    "    mcl = train_data['type'].value_counts().idxmax()\n",
    "\n",
    "    # Make predictions for training and validation sets\n",
    "    train_data['predicted'] = mcl\n",
    "    val_data['predicted'] = mcl\n",
    "\n",
    "    # Calculate accuracy\n",
    "    train_accuracy = (train_data['type'] == train_data['predicted']).mean()\n",
    "    val_accuracy = (val_data['type'] == val_data['predicted']).mean()\n",
    "\n",
    "    return train_accuracy, val_accuracy\n",
    "\n",
    "# Run MCL model on data\n",
    "train_accuracy, val_accuracy = MCL(train_data, val_data)\n",
    "\n",
    "# Print results\n",
    "print(\"Most Common Label model:\")\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Validation accuracy: {val_accuracy:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     data \u001b[39m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[39m# run the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     data \u001b[39m=\u001b[39m RandomForest(training_data, validation_data)\n\u001b[0;32m     45\u001b[0m \u001b[39mprint\u001b[39m(data)\n",
      "Cell \u001b[1;32mIn[57], line 12\u001b[0m, in \u001b[0;36mRandomForest\u001b[1;34m(training_data, validation_data)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mRandomForest\u001b[39m(training_data, validation_data):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# vectorize the training data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     vectorizer \u001b[39m=\u001b[39m CountVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, max_features\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[1;32m---> 12\u001b[0m     X_train_vect \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(training_data[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mU\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     13\u001b[0m     y_train \u001b[39m=\u001b[39m training_data[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     15\u001b[0m     \u001b[39m# vectorize the validation data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1401\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1400\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[1;32m-> 1401\u001b[0m X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_limit_features(\n\u001b[0;32m   1402\u001b[0m     X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[0;32m   1403\u001b[0m )\n\u001b[0;32m   1404\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[1;32mc:\\Users\\caspe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1250\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1249\u001b[0m         \u001b[39mdel\u001b[39;00m vocabulary[term]\n\u001b[1;32m-> 1250\u001b[0m         removed_terms\u001b[39m.\u001b[39;49madd(term)\n\u001b[0;32m   1251\u001b[0m kept_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(mask)[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kept_indices) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def RandomForest(training_data, validation_data):\n",
    "    # vectorize the training data\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=5000, ngram_range=(1,2))\n",
    "    X_train_vect = vectorizer.fit_transform(training_data['content'].values.astype('U'))\n",
    "    y_train = training_data['content'].values\n",
    "    \n",
    "    # vectorize the validation data\n",
    "    X_val_vect = vectorizer.transform(validation_data['content'].values.astype('U'))\n",
    "    y_val = validation_data['content'].values\n",
    "\n",
    "    # initialize and fit the random forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_vect.toarray(), y_train)\n",
    "    \n",
    "    # make predictions and calculate accuracy\n",
    "    y_pred = rf.predict(X_val_vect.toarray())\n",
    "    accuracy = (y_pred == y_val).mean()\n",
    "\n",
    "    return {'model': rf, 'vectorizer': vectorizer, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "\n",
    "# split the data into non-null rows\n",
    "training_data = train_data.dropna(subset=['content'])\n",
    "validation_data = val_data.dropna(subset=['content'])\n",
    "\n",
    "# check that we have at least one non-null row in each set\n",
    "if training_data.shape[0] == 0:\n",
    "    raise ValueError(\"Training data is empty!\")\n",
    "if validation_data.shape[0] == 0:\n",
    "    print(\"Warning: Validation data is empty. Setting predictions to empty list.\")\n",
    "    data = []\n",
    "else:\n",
    "    # run the model\n",
    "    data = RandomForest(training_data, validation_data)\n",
    "    \n",
    "print(data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa65be42c2d5f30e8678b08aab0206abc6098dbb196d65557b8dc168e4f7f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
